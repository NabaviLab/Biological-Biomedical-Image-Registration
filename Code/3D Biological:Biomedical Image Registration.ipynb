{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Require Libraries"
      ],
      "metadata": {
        "id": "a9hRiQYtmoOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LIBRARY IMPORTS AND INSTALLATION INSTRUCTIONS\n",
        "\n",
        "# To ensure that the code can run properly, please install the following packages using pip or conda:\n",
        "\n",
        "# !pip install opencv-python-headless numpy torch torchvision pillow scipy pandas matplotlib seaborn plotly scikit-image psutil scikit-learn\n",
        "\n",
        "# Essential Libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "\n",
        "# Statistical Analysis\n",
        "from scipy.signal import correlate2d\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "# Error Handling\n",
        "import warnings\n",
        "\n",
        "# Performance Measurements\n",
        "import time\n",
        "from psutil import virtual_memory\n",
        "\n",
        "# Image Analysis\n",
        "from skimage import io\n",
        "from skimage.metrics import normalized_root_mse as ncc, structural_similarity as ssim\n",
        "from scipy.ndimage.filters import gaussian_gradient_magnitude\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.base import BaseEstimator"
      ],
      "metadata": {
        "id": "wZHvKLRloqF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlhH8Zsymk5Y"
      },
      "outputs": [],
      "source": [
        "# This line of code is used to set the device for computation.\n",
        "# If a CUDA-compatible GPU (Graphics Processing Unit) is available on the system,\n",
        "# it will be set as the default device for tensor computations, which can significantly \n",
        "# speed up deep learning operations. If a CUDA-compatible GPU is not available, \n",
        "# the computations will default to the system's CPU (Central Processing Unit).\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# This function defines the likelihood of a model given the data.\n",
        "# This function uses a Gaussian (also known as Normal) distribution.\n",
        "# The Gaussian distribution is parameterized by its mean and standard deviation (sigma).\n",
        "# In this case, the 'error' variable represents the difference between the data and the model's predictions.\n",
        "# The function returns a measure of the likelihood that the model generated the data.\n",
        "# If the error is small, the function will return a large value (indicating high likelihood), and vice versa.\n",
        "# \n",
        "# Input Parameters:\n",
        "# - error: The difference between the observed data and the model's predictions.\n",
        "# - sigma: The standard deviation of the Gaussian distribution. It's a measure of the variability in the data.\n",
        "#\n",
        "# Returns:\n",
        "# - likelihood: A measure of how likely it is that the model generated the observed data.\n",
        "\n",
        "def likelihood(error, sigma):\n",
        "    return np.exp(-error**2 / (2 * sigma**2))\n",
        "\n",
        "\n",
        "# This class defines a Homography Model which inherits from the BaseEstimator class provided by scikit-learn.\n",
        "# Homography is a transformation that maps the points in one image to the corresponding points in the other image.\n",
        "# A homography matrix is a 3x3 matrix that performs this transformation.\n",
        "# The HomographyModel has two main methods:\n",
        "# 1. fit: This method calculates the homography matrix between source points and destination points.\n",
        "# 2. errors: This method calculates the reprojection error, which is the Euclidean distance between\n",
        "#    the destination points and the source points transformed by the estimated homography matrix.\n",
        "#\n",
        "class HomographyModel(BaseEstimator):\n",
        "    # The fit method estimates the homography matrix using RANSAC algorithm\n",
        "    # \n",
        "    # Input Parameters:\n",
        "    # - X: The source and destination points. The first half should be the source points, \n",
        "    #      and the second half should be the destination points.\n",
        "    #\n",
        "    # Returns:\n",
        "    # - self: An instance of the class HomographyModel.\n",
        "    def fit(self, X, y=None):\n",
        "        src_pts = X[:, :2].reshape(-1, 1, 2)\n",
        "        dst_pts = X[:, 2:].reshape(-1, 1, 2)\n",
        "        self.H, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC)\n",
        "        return self\n",
        "\n",
        "    # The errors method calculates the reprojection error, which is the Euclidean distance between\n",
        "    # the destination points and the source points transformed by the estimated homography matrix.\n",
        "    #\n",
        "    # Input Parameters:\n",
        "    # - X: The source and destination points. The first half should be the source points, \n",
        "    #      and the second half should be the destination points.\n",
        "    #\n",
        "    # Returns:\n",
        "    # - A 1D numpy array containing the reprojection errors for each point.\n",
        "    def errors(self, X, y=None):\n",
        "        src_pts = X[:, :2].reshape(-1, 1, 2)\n",
        "        dst_pts = X[:, 2:].reshape(-1, 1, 2)\n",
        "        dst_pts_estimated = cv2.perspectiveTransform(src_pts, self.H)\n",
        "        return np.sqrt(np.sum((dst_pts - dst_pts_estimated)**2, axis=2)).ravel()\n",
        "\n",
        "\n",
        "\n",
        "# This function implements the MLESAC algorithm, which is a robust method for fitting a model\n",
        "# to data that may contain outliers. MLESAC uses maximum likelihood estimation to evaluate the quality \n",
        "# of the fitted models, instead of the consensus set size used by RANSAC. It allows to find \n",
        "# the best model with the highest likelihood.\n",
        "#\n",
        "# The mlesac function:\n",
        "# 1. Randomly selects a subset of the data and fits the model to this subset.\n",
        "# 2. Calculates the errors of the data points to the fitted model.\n",
        "# 3. Classifies the data points as inliers or outliers based on a threshold.\n",
        "# 4. Calculates the likelihood of the inliers and the outliers.\n",
        "# 5. If the total likelihood is higher than the best likelihood found so far, updates the best model, \n",
        "#    the best likelihood, and the best inliers.\n",
        "# 6. Repeats the process for a specified number of iterations.\n",
        "#\n",
        "# Input Parameters:\n",
        "# - data: The input data.\n",
        "# - model_class: The class of the model to be fitted.\n",
        "# - min_samples: The minimum number of data points required to fit the model.\n",
        "# - max_iterations: The maximum number of iterations for the algorithm.\n",
        "# - sigma: The standard deviation of the Gaussian that is used to estimate the likelihood.\n",
        "# - threshold: The threshold used to classify the data points as inliers or outliers.\n",
        "#\n",
        "# Returns:\n",
        "# - best_model: The best fitted model.\n",
        "# - best_inliers: The inliers of the best fitted model.\n",
        "\n",
        "def mlesac(data, model_class, min_samples, max_iterations, sigma, threshold):\n",
        "    best_model = None\n",
        "    best_likelihood = 0\n",
        "    best_inliers = None\n",
        "\n",
        "    for _ in range(max_iterations):\n",
        "        # Select a random subset of the data\n",
        "        sample = data[np.random.choice(data.shape[0], min_samples, replace=False)]\n",
        "        # Fit the model to the subset\n",
        "        model = model_class()\n",
        "        model.fit(sample)\n",
        "\n",
        "        # Calculate the errors of the data points to the fitted model\n",
        "        errors = model.errors(data)\n",
        "        # Classify the data points as inliers or outliers based on the threshold\n",
        "        inliers = data[errors <= threshold]\n",
        "        outliers = data[errors > threshold]\n",
        "\n",
        "        # Calculate the likelihood of the inliers and the outliers\n",
        "        likelihood_inliers = np.sum(likelihood(errors[errors <= threshold], sigma))\n",
        "        likelihood_outliers = np.sum(likelihood(errors[errors > threshold], sigma))\n",
        "        # Calculate the total likelihood\n",
        "        likelihood_total = likelihood_inliers + likelihood_outliers\n",
        "\n",
        "        # If the total likelihood is higher than the best likelihood found so far, update the best model, \n",
        "        # the best likelihood, and the best inliers\n",
        "        if likelihood_total > best_likelihood:\n",
        "            best_model = model\n",
        "            best_likelihood = likelihood_total\n",
        "            best_inliers = inliers\n",
        "\n",
        "    # Return the best model and the inliers of the best model\n",
        "    return best_model, best_inliers\n",
        "\n",
        "\n",
        "# The draw_keypoint_matches function visualizes the matching keypoints between two images.\n",
        "# It takes as input two images, their keypoints, the matches between the keypoints, an adaptive threshold, \n",
        "# and a path to save the resulting image. The function draws only the matches that have a distance \n",
        "# less than or equal to the adaptive threshold.\n",
        "# \n",
        "# Input Parameters:\n",
        "# - img1: The first image.\n",
        "# - img2: The second image.\n",
        "# - keypoints1: The keypoints detected in the first image.\n",
        "# - keypoints2: The keypoints detected in the second image.\n",
        "# - matches: The matches between the keypoints of the two images.\n",
        "# - adaptive_threshold: The threshold used to filter the matches. Only matches with a distance \n",
        "#   less than or equal to this threshold are drawn.\n",
        "# - save_path: The path where the resulting image is saved.\n",
        "# \n",
        "# Returns:\n",
        "# - None. The function saves the resulting image to the specified path.\n",
        "\n",
        "def draw_keypoint_matches(img1, img2, keypoints1, keypoints2, matches, adaptive_threshold, save_path):\n",
        "    # Filter the matches based on the adaptive threshold\n",
        "    good_matches = [m for m in matches if m.distance <= adaptive_threshold]\n",
        "    # Draw the matches on the images\n",
        "    img_matches = cv2.drawMatches(img1, keypoints1, img2, keypoints2, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
        "    # Save the image with drawn matches\n",
        "    cv2.imwrite(save_path, img_matches)\n",
        "\n",
        "\n",
        "# The adjust_brightness function adjusts the brightness of an image.\n",
        "# It takes as input an image and optionally the scale factor (alpha) and an added constant (beta).\n",
        "# \n",
        "# Input Parameters:\n",
        "# - img: The input image.\n",
        "# - alpha: The scale factor that is multiplied with each pixel. Default is 0.0.\n",
        "# - beta: The constant added to each pixel. Default is 0.0.\n",
        "# \n",
        "# Returns:\n",
        "# - Adjusted image with the new brightness.\n",
        "def adjust_brightness(img, alpha=0.0, beta=0.0):\n",
        "    return cv2.convertScaleAbs(img, alpha=alpha, beta=beta)\n",
        "\n",
        "\n",
        "# The adjust_contrast function adjusts the contrast of an image.\n",
        "# It takes as input an image and optionally the scale factor (alpha) and an added constant (beta).\n",
        "# \n",
        "# Input Parameters:\n",
        "# - img: The input image.\n",
        "# - alpha: The scale factor that is multiplied with each pixel. Default is 0.0.\n",
        "# - beta: The constant added to each pixel. Default is 0.0.\n",
        "# \n",
        "# Returns:\n",
        "# - Adjusted image with the new contrast.\n",
        "def adjust_contrast(img, alpha=0.0, beta=0.0):\n",
        "    return cv2.convertScaleAbs(img, alpha=alpha, beta=beta)\n",
        "\n",
        "\n",
        "# The denoise_image function denoises an image using the Non-local Means Denoising algorithm.\n",
        "# It takes as input an image and optionally a weight for the denoising process.\n",
        "# \n",
        "# Input Parameters:\n",
        "# - img: The input image.\n",
        "# - weight: The weight for the denoising process. Default is 0.1.\n",
        "# \n",
        "# Returns:\n",
        "# - Denoised image.\n",
        "def denoise_image(img, weight=0.1):\n",
        "    return cv2.fastNlMeansDenoising(img, None, weight)\n",
        "\n",
        "\n",
        "# The preprocess_image function applies a series of preprocessing steps to an image.\n",
        "# It first adjusts the brightness, then the contrast, and finally denoises the image.\n",
        "# \n",
        "# Input Parameters:\n",
        "# - img: The input image.\n",
        "# \n",
        "# Returns:\n",
        "# - The preprocessed image.\n",
        "def preprocess_image(img):\n",
        "    img = adjust_brightness(img, alpha=1.0, beta=0.0)  # Adjust brightness\n",
        "    img = adjust_contrast(img, alpha=1.0, beta=0.0)    # Adjust contrast\n",
        "    img = denoise_image(img, weight=0.0)               # Denoise image\n",
        "    return img\n",
        "\n",
        "\n",
        "# The open_images function opens a fixed image and a list of moving images using their file paths.\n",
        "# It applies preprocessing steps to all images using the preprocess_image function.\n",
        "#\n",
        "# Input Parameters:\n",
        "# - fixed_image_path: The file path of the fixed image.\n",
        "# - moving_image_paths: A list of file paths for the moving images.\n",
        "#\n",
        "# Returns:\n",
        "# - The preprocessed fixed image and a list of preprocessed moving images.\n",
        "def open_images(fixed_image_path, moving_image_paths):\n",
        "    # Read the fixed image from the file path\n",
        "    fixed_image = cv2.imread(fixed_image_path)\n",
        "    \n",
        "    # Read the moving images from the file paths\n",
        "    moving_images = [cv2.imread(path) for path in moving_image_paths]\n",
        "\n",
        "    # Apply preprocessing to the fixed image\n",
        "    fixed_image = preprocess_image(fixed_image)\n",
        "    \n",
        "    # Apply preprocessing to the moving images\n",
        "    moving_images = [preprocess_image(img) for img in moving_images]\n",
        "\n",
        "    return fixed_image, moving_images\n",
        "\n",
        "\n",
        "# The preprocess_image_for_dl function processes an image for input into a deep learning model.\n",
        "# It converts the image to RGB, applies a series of transforms, and converts the image to a tensor.\n",
        "#\n",
        "# Input Parameters:\n",
        "# - img: The input image.\n",
        "#\n",
        "# Returns:\n",
        "# - The preprocessed image tensor.\n",
        "def preprocess_image_for_dl(img):\n",
        "    # Convert the input image to a PIL Image object\n",
        "    img = Image.fromarray(img)\n",
        "    \n",
        "    # Convert the image to RGB format\n",
        "    img = img.convert('RGB')\n",
        "    \n",
        "    # Define a series of transforms:\n",
        "    # Resize and center crop the image to 224x224 pixels,\n",
        "    # Convert the image to a tensor,\n",
        "    # Normalize the image with mean and standard deviation for each color channel.\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    \n",
        "    # Apply the defined transforms to the image\n",
        "    img_tensor = preprocess(img)\n",
        "\n",
        "    return img_tensor\n",
        "\n",
        "\n",
        "# The features_to_keypoints_and_descriptors function takes an array of features as input,\n",
        "# reshapes it into a 2D image, and then uses the SIFT feature detection algorithm\n",
        "# to extract keypoints and descriptors from the image.\n",
        "#\n",
        "# Input Parameters:\n",
        "# - features_np: A numpy array of features.\n",
        "#\n",
        "# Returns:\n",
        "# - keypoints: The detected keypoints.\n",
        "# - descriptors: The corresponding descriptors for the detected keypoints.\n",
        "def features_to_keypoints_and_descriptors(features_np):\n",
        "    # Reshape the input array into a 2D image.\n",
        "    # The chosen dimensions are the square root of the size of the input array,\n",
        "    # ensuring that the total number of elements remains the same.\n",
        "    h, w = int(np.sqrt(features_np.size)), int(np.sqrt(features_np.size))\n",
        "    img = cv2.resize(features_np, (h, w))\n",
        "\n",
        "    # Convert the image to 8-bit unsigned integers.\n",
        "    # This is done for compatibility with the SIFT feature detector, which requires 8-bit images.\n",
        "    img = (img * 255).astype(np.uint8)\n",
        "\n",
        "    # Initialize the SIFT feature detector.\n",
        "    sift = cv2.SIFT_create()\n",
        "\n",
        "    # Use the SIFT feature detector to detect keypoints in the image,\n",
        "    # and compute the corresponding descriptors.\n",
        "    keypoints, descriptors = sift.detectAndCompute(img, None)\n",
        "\n",
        "    return keypoints, descriptors\n",
        "\n",
        "\n",
        "\n",
        "# The extract_features function uses both a deep learning model (ResNet) and the SIFT algorithm\n",
        "# to extract features from a given image.\n",
        "#\n",
        "# Input Parameters:\n",
        "# - img: The image from which features will be extracted.\n",
        "# - model: The deep learning model (ResNet) used for feature extraction.\n",
        "#\n",
        "# Returns:\n",
        "# - kp: The detected keypoints.\n",
        "# - des: The corresponding descriptors for the detected keypoints.\n",
        "def extract_features(img, model):\n",
        "    # Preprocess the image for deep learning, add an extra dimension to the tensor,\n",
        "    # and move it to the device where the computations will be performed.\n",
        "    img_tensor = preprocess_image_for_dl(img).unsqueeze(0).to(device)\n",
        "    \n",
        "    # Use the ResNet model to extract features from the image.\n",
        "    features = model(img_tensor)\n",
        "    \n",
        "    # Remove the extra dimension from the features tensor, move it back to the CPU,\n",
        "    # and convert it into a numpy array.\n",
        "    features_np = features.squeeze().cpu().detach().numpy()\n",
        "    \n",
        "    # Convert the numpy array of features into float32 type for compatibility with the SIFT function.\n",
        "    features_np = features_np.astype(np.float32)\n",
        "    \n",
        "    # Use the SIFT function to detect keypoints and compute descriptors based on the extracted features.\n",
        "    kp, des = features_to_keypoints_and_descriptors(features_np)\n",
        "    \n",
        "    return kp, des\n",
        "\n",
        "\n",
        "# The extract_combined_features function uses both a deep learning model (ResNet) and the SIFT algorithm\n",
        "# to extract features from a given image, effectively combining both methods.\n",
        "#\n",
        "# Input Parameters:\n",
        "# - image: The image from which features will be extracted.\n",
        "# - model: The deep learning model (ResNet) used for feature extraction.\n",
        "#\n",
        "# Returns:\n",
        "# - resnet_keypoints: The keypoints detected by the ResNet model.\n",
        "# - resnet_descriptors: The corresponding descriptors for the ResNet detected keypoints.\n",
        "# - sift_keypoints: The keypoints detected by the SIFT algorithm.\n",
        "# - sift_descriptors: The corresponding descriptors for the SIFT detected keypoints.\n",
        "def extract_combined_features(image, model):\n",
        "    # Extract features using the deep learning model (ResNet).\n",
        "    resnet_keypoints, resnet_descriptors = extract_features(image, model)\n",
        "    \n",
        "    # Initialize the SIFT feature detector.\n",
        "    sift = cv2.SIFT_create()\n",
        "    \n",
        "    # Use the SIFT feature detector to detect keypoints and compute descriptors.\n",
        "    sift_keypoints, sift_descriptors = sift.detectAndCompute(image, None)\n",
        "    \n",
        "    return resnet_keypoints, resnet_descriptors, sift_keypoints, sift_descriptors\n",
        "\n",
        "\n",
        "\n",
        "# The homography_registration function performs image registration by finding a homography (a transformation that maps points in one image to another) \n",
        "# using combined features from the deep learning model (ResNet) and the SIFT algorithm.\n",
        "#\n",
        "# Input Parameters:\n",
        "# - img1, img2: The pair of images to be registered.\n",
        "# - model: The deep learning model (ResNet) used for feature extraction.\n",
        "# - method: The method used to compute the homography (default is RANSAC).\n",
        "# - threshold: The distance threshold to identify inliers in the RANSAC algorithm.\n",
        "# - threshold_multiplier: A factor to adjust the adaptive threshold. \n",
        "#\n",
        "# Returns:\n",
        "# - H: The homography matrix.\n",
        "# - matches: The matched keypoints from both images.\n",
        "# - adaptive_threshold: The adaptive threshold used to select good matches.\n",
        "# - keypoints1, keypoints2: The keypoints detected in each image.\n",
        "def homography_registration(img1, img2, model, method=cv2.RANSAC, threshold=5.0, threshold_multiplier=5.0):\n",
        "    # Extract combined features from both images.\n",
        "    resnet_keypoints1, resnet_descriptors1, keypoints1, descriptors1 = extract_combined_features(img1, model)\n",
        "    resnet_keypoints2, resnet_descriptors2, keypoints2, descriptors2 = extract_combined_features(img2, model)\n",
        "\n",
        "    # Initialize the FLANN-based matcher for SIFT features.\n",
        "    FLANN_INDEX_KDTREE = 1\n",
        "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
        "    search_params = dict(checks=50)\n",
        "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "    \n",
        "    # Match the SIFT features from both images.\n",
        "    sift_matches = flann.knnMatch(descriptors1, descriptors2, k=2)\n",
        "\n",
        "    # Apply ratio test to find good matches.\n",
        "    good_sift_matches = []\n",
        "    for m, n in sift_matches:\n",
        "        if m.distance < 0.7 * n.distance:\n",
        "            good_sift_matches.append(m)\n",
        "\n",
        "    # Initialize a brute-force matcher for the ResNet features.\n",
        "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
        "    \n",
        "    # Match the ResNet features from both images.\n",
        "    resnet_matches = bf.match(resnet_descriptors1, resnet_descriptors2)\n",
        "\n",
        "    # Combine the matches from both ResNet and SIFT.\n",
        "    matches = good_sift_matches + list(resnet_matches)\n",
        "    \n",
        "    # Sort the matches based on their distance.\n",
        "    matches = sorted(matches, key=lambda x: x.distance)\n",
        "\n",
        "    # Calculate the median distance among the matches and use it to set an adaptive threshold.\n",
        "    median_distance = np.median([m.distance for m in matches])\n",
        "    adaptive_threshold = median_distance * threshold_multiplier\n",
        "\n",
        "    # Prepare the data for the MLESAC algorithm.\n",
        "    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
        "    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
        "    data = np.hstack([src_pts.reshape(-1, 2), dst_pts.reshape(-1, 2)])\n",
        "\n",
        "    # Compute the homography using the MLESAC algorithm.\n",
        "    model, inliers = mlesac(data, HomographyModel, min_samples=4, max_iterations=1000, sigma=1.0, threshold=adaptive_threshold)\n",
        "    H = model.H\n",
        "\n",
        "    print(f\"Number of key points found: {len(matches)}\")\n",
        "    print(f\"Number of key points found: {len(matches)}\")\n",
        "    print(f\"Adaptive threshold used: {adaptive_threshold}\")\n",
        "\n",
        "    # Return the computed homography matrix, the matched keypoints, the adaptive threshold used and the detected keypoints in each image.\n",
        "    return H, matches, adaptive_threshold, keypoints1, keypoints2\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Define the paths to the fixed image and the moving images\n",
        "    fixed_image_path = 'replace with your fixed image path'\n",
        "    moving_image_paths = ['replace with your moving image(s) path']\n",
        "\n",
        "    # Load and preprocess the images\n",
        "    fixed_image, moving_images = open_images(fixed_image_path, moving_image_paths)\n",
        "\n",
        "    # Define the shape of the image and the threshold multiplier\n",
        "    height, width = fixed_image.shape[:2]\n",
        "    threshold_multiplier = 1\n",
        "\n",
        "    # Load the pre-trained ResNet50 model, and remove its last layer to get the features\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
        "\n",
        "    # Iterate over each moving image and register it with the fixed image\n",
        "    for idx, img2 in enumerate(moving_images):\n",
        "\n",
        "        # Start timer to calculate registration time\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Calculate the homography registration, the adaptive threshold, and get the keypoints for each image\n",
        "        H, matches, adaptive_threshold, keypoints1, keypoints2 = homography_registration(fixed_image, img2, model, threshold_multiplier=threshold_multiplier)\n",
        "        \n",
        "        # Apply the computed homography transformation to the moving image\n",
        "        result = cv2.warpPerspective(img2, H, (width, height), flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP)\n",
        "\n",
        "        # Stop timer and calculate registration time\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Calculate number of inliers for adaptive threshold\n",
        "        inliers = np.sum([1 for m in matches if m.distance <= adaptive_threshold])\n",
        "        Registration_time = end_time - start_time\n",
        "\n",
        "        # Print the results\n",
        "        print(\"Registration_time (s): \", Registration_time)\n",
        "        print(f\"Image {idx + 1}: Number of inliers with adaptive threshold: {inliers}\")\n",
        "\n",
        "        # Save the result image\n",
        "        cv2.imwrite(f'replace with your result image path', result)\n",
        "        \n",
        "        # Calculate and print number of inliers for fixed threshold\n",
        "        fixed_threshold = 20\n",
        "        inliers = np.sum([1 for m in matches if m.distance <= fixed_threshold])\n",
        "        print(f\"Image {idx + 1}: Number of inliers without adaptive threshold: {inliers}\")\n",
        "\n",
        "        # Save an image with the matched keypoints\n",
        "        draw_keypoint_matches(fixed_image, img2, keypoints1, keypoints2, matches, adaptive_threshold, f'replace with your image path')\n",
        "\n"
      ]
    }
  ]
}